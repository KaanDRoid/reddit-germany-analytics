{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828e98b5",
   "metadata": {},
   "source": [
    "# Reddit Real-Time Analytics - Consumer (Spark Streaming)\n",
    "\n",
    "**Team Members:** Alina Insam, Sumedh Bamane, Rafael Machado Da Rocha, Kaan Ak\n",
    "\n",
    "**Project Description:** This consumer receives Germany-related Reddit comments via socket, processes them with Spark Streaming to extract insights including:\n",
    "- TF-IDF analysis for important words\n",
    "- Reference extraction (users, subreddits, URLs)\n",
    "- Sentiment analysis\n",
    "- Real-time metrics and statistics\n",
    "\n",
    "**Architecture:**\n",
    "1. Receive streaming data via socket\n",
    "2. Store raw data in Spark table \"raw\"\n",
    "3. Process data for analytics\n",
    "4. Store results in Spark table \"metrics\"\n",
    "5. Save results to disk\n",
    "6. Generate visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205adfe-4a90-4eb8-969c-76b7a1f40a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc244322-8c53-465c-bdb5-c6614cb823a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/29 09:06:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# 1. Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditStreamConsumer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdefd171-249b-4ba1-be49-ca19a9d6f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType\n",
    "\n",
    "# Create Spark Session with optimized configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditGermanyAnalytics\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"‚úÖ Spark Session created: {spark.version}\")\n",
    "print(f\"üîß Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"text\", StringType()) \\\n",
    "    .add(\"created_utc\", DoubleType()) \\\n",
    "    .add(\"link\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38d6ad-4fc1-4d38-9068-bec315736f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 09:06:29 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    "\n",
    "# Define comprehensive schema for incoming Reddit data\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"created_utc\", DoubleType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"subreddit\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"link\", StringType(), True),\n",
    "    StructField(\"user_mentions\", ArrayType(StringType()), True),\n",
    "    StructField(\"subreddit_references\", ArrayType(StringType()), True),\n",
    "    StructField(\"urls\", ArrayType(StringType()), True),\n",
    "    StructField(\"sentiment\", StructType([\n",
    "        StructField(\"compound\", DoubleType(), True),\n",
    "        StructField(\"positive\", DoubleType(), True),\n",
    "        StructField(\"negative\", DoubleType(), True),\n",
    "        StructField(\"neutral\", DoubleType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"word_count\", IntegerType(), True),\n",
    "    StructField(\"char_count\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"üìã Schema defined with all fields from producer\")\n",
    "\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"host.docker.internal\") \\\n",
    "    .option(\"port\", 9998) \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb73dc-3a8d-4322-b79b-195607abc5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read streaming data from socket\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9998) \\\n",
    "    .option(\"includeTimestamp\", \"true\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"üîå Socket stream configured (localhost:9998)\")\n",
    "print(\"‚ö†Ô∏è  Make sure the producer is running before starting the stream!\")\n",
    "\n",
    "# 4. Parse JSON strings from producer\n",
    "json_df = raw_stream.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b6b81-5bb4-41ef-9eec-d2c41ecb4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, from_unixtime, monotonically_increasing_id\n",
    "\n",
    "# 5. Print only if there is data\n",
    "def process_batch(df, epoch_id):\n",
    "    count = df.count()\n",
    "    if count > 0:\n",
    "        print(f\"\\n‚è∞ Received {count} comment(s) in batch {epoch_id}\")\n",
    "        df.show(truncate=False)\n",
    "\n",
    "# Parse JSON data and add processing timestamp\n",
    "parsed_df = raw_stream.select(\n",
    "    from_json(col(\"value\"), schema).alias(\"data\"),\n",
    "    col(\"timestamp\").alias(\"processing_time\")\n",
    ").select(\"data.*\", \"processing_time\")\n",
    "\n",
    "# Add additional computed columns\n",
    "processed_df = parsed_df.withColumn(\"created_datetime\", \n",
    "                                   from_unixtime(col(\"created_utc\")).cast(\"timestamp\")) \\\n",
    "                      .withColumn(\"processing_datetime\", \n",
    "                                 col(\"processing_time\").cast(\"timestamp\")) \\\n",
    "                      .withColumn(\"batch_id\", monotonically_increasing_id())\n",
    "\n",
    "print(\"üîÑ JSON parsing and timestamp processing configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae640dc8-6d51-4c10-b3f0-57ce5486ebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 09:06:35 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9ad2e73a-5ea1-4906-b2a6-95983b7b41ae. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 09:06:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/29 09:06:36 ERROR MicroBatchExecution: Query [id = 0c9f0ba2-1c19-40e9-92b3-173488d4c142, runId = 8aa6425c-7b63-4f3e-bda1-4b3e513c9e3f] terminated with error\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.TextSocketMicroBatchStream.initialize(TextSocketMicroBatchStream.scala:71)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.TextSocketMicroBatchStream.planInputPartitions(TextSocketMicroBatchStream.scala:117)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:46)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:46)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:179)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:157)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:496)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:186)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:186)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:720)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:708)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 0c9f0ba2-1c19-40e9-92b3-173488d4c142, runId = 8aa6425c-7b63-4f3e-bda1-4b3e513c9e3f] terminated with exception: Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStreamingQueryException\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 6. Start stream\u001b[39;00m\n\u001b[32m      2\u001b[39m query = json_df.writeStream \\\n\u001b[32m      3\u001b[39m     .foreachBatch(process_batch) \\\n\u001b[32m      4\u001b[39m     .outputMode(\u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      5\u001b[39m     .start()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/streaming/query.py:221\u001b[39m, in \u001b[36mStreamingQuery.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jsq.awaitTermination(\u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m))\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mStreamingQueryException\u001b[39m: [STREAM_FAILED] Query [id = 0c9f0ba2-1c19-40e9-92b3-173488d4c142, runId = 8aa6425c-7b63-4f3e-bda1-4b3e513c9e3f] terminated with exception: Connection refused"
     ]
    }
   ],
   "source": [
    "# 6. Start stream\n",
    "# Setup buffers and output directories before starting\n",
    "raw_data_buffer = []\n",
    "metrics_buffer = []\n",
    "batch_counter = 0\n",
    "os.makedirs(\"output/raw_data\", exist_ok=True)\n",
    "os.makedirs(\"output/metrics\", exist_ok=True)\n",
    "os.makedirs(\"output/visualizations\", exist_ok=True)\n",
    "\n",
    "# Start streaming query on processed DataFrame\n",
    "query = processed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"./checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "# Keep the query running\n",
    "query.awaitTermination()\n",
    "\n",
    "# Global variables for storing data\n",
    "raw_data_buffer = []\n",
    "metrics_buffer = []\n",
    "batch_counter = 0\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"output/raw_data\", exist_ok=True)\n",
    "os.makedirs(\"output/metrics\", exist_ok=True)\n",
    "os.makedirs(\"output/visualizations\", exist_ok=True)\n",
    "\n",
    "def extract_top_words_tfidf(texts, top_n=10):\n",
    "    \"\"\"Extract top N words using TF-IDF\"\"\"\n",
    "    try:\n",
    "        if not texts or len(texts) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Create DataFrame from texts\n",
    "        text_df = spark.createDataFrame([(i, text) for i, text in enumerate(texts)], \n",
    "                                       [\"id\", \"text\"])\n",
    "        \n",
    "        # TF-IDF Pipeline\n",
    "        tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "        remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "        hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"tf_features\", numFeatures=1000)\n",
    "        idf = IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\")\n",
    "        \n",
    "        pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "        model = pipeline.fit(text_df)\n",
    "        result = model.transform(text_df)\n",
    "        \n",
    "        # Extract top words (simplified approach)\n",
    "        words_freq = {}\n",
    "        for row in result.select(\"filtered_words\").collect():\n",
    "            for word in row.filtered_words:\n",
    "                if len(word) > 2:  # Skip short words\n",
    "                    words_freq[word] = words_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Return top N words\n",
    "        top_words = sorted(words_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        return top_words\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå TF-IDF Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_references(df_batch):\n",
    "    \"\"\"Analyze user mentions, subreddit references, and URLs\"\"\"\n",
    "    try:\n",
    "        data = df_batch.collect()\n",
    "        \n",
    "        user_mentions = {}\n",
    "        subreddit_refs = {}\n",
    "        url_domains = {}\n",
    "        \n",
    "        for row in data:\n",
    "            # Count user mentions\n",
    "            if row.user_mentions:\n",
    "                for user in row.user_mentions:\n",
    "                    user_mentions[user] = user_mentions.get(user, 0) + 1\n",
    "            \n",
    "            # Count subreddit references\n",
    "            if row.subreddit_references:\n",
    "                for sub in row.subreddit_references:\n",
    "                    subreddit_refs[sub] = subreddit_refs.get(sub, 0) + 1\n",
    "            \n",
    "            # Count URL domains\n",
    "            if row.urls:\n",
    "                for url in row.urls:\n",
    "                    try:\n",
    "                        domain = url.split('//')[1].split('/')[0]\n",
    "                        url_domains[domain] = url_domains.get(domain, 0) + 1\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        return {\n",
    "            'top_user_mentions': sorted(user_mentions.items(), key=lambda x: x[1], reverse=True)[:10],\n",
    "            'top_subreddit_refs': sorted(subreddit_refs.items(), key=lambda x: x[1], reverse=True)[:10],\n",
    "            'top_url_domains': sorted(url_domains.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reference analysis error: {e}\")\n",
    "        return {'top_user_mentions': [], 'top_subreddit_refs': [], 'top_url_domains': []}\n",
    "\n",
    "def calculate_metrics(df_batch, batch_id):\n",
    "    \"\"\"Calculate comprehensive metrics for the batch\"\"\"\n",
    "    global raw_data_buffer, metrics_buffer, batch_counter\n",
    "    \n",
    "    try:\n",
    "        batch_counter += 1\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # Collect batch data\n",
    "        batch_data = df_batch.collect()\n",
    "        \n",
    "        if not batch_data:\n",
    "            print(f\"üì≠ Batch {batch_id}: No data received\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüîÑ Processing Batch {batch_id} - {len(batch_data)} comments\")\n",
    "        print(f\"‚è∞ Timestamp: {current_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Add to raw data buffer\n",
    "        raw_data_buffer.extend(batch_data)\n",
    "        \n",
    "        # Keep only last 1000 records to manage memory\n",
    "        if len(raw_data_buffer) > 1000:\n",
    "            raw_data_buffer = raw_data_buffer[-1000:]\n",
    "        \n",
    "        # Basic statistics\n",
    "        texts = [row.text for row in batch_data if row.text]\n",
    "        sentiments = [row.sentiment.compound for row in batch_data if row.sentiment]\n",
    "        scores = [row.score for row in batch_data if row.score is not None]\n",
    "        subreddits = [row.subreddit for row in batch_data if row.subreddit]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0\n",
    "        avg_score = sum(scores) / len(scores) if scores else 0\n",
    "        total_words = sum([row.word_count for row in batch_data if row.word_count])\n",
    "        \n",
    "        # Sentiment distribution\n",
    "        positive_comments = len([s for s in sentiments if s > 0.1])\n",
    "        negative_comments = len([s for s in sentiments if s < -0.1])\n",
    "        neutral_comments = len([s for s in sentiments if -0.1 <= s <= 0.1])\n",
    "        \n",
    "        # Subreddit distribution\n",
    "        subreddit_counts = {}\n",
    "        for sub in subreddits:\n",
    "            subreddit_counts[sub] = subreddit_counts.get(sub, 0) + 1\n",
    "        \n",
    "        # Get TF-IDF top words from all accumulated data\n",
    "        all_texts = [row.text for row in raw_data_buffer if row.text]\n",
    "        top_words_tfidf = extract_top_words_tfidf(all_texts[-500:])  # Use last 500 texts\n",
    "        \n",
    "        # Analyze references\n",
    "        references = analyze_references(df_batch)\n",
    "        \n",
    "        # Time range\n",
    "        created_times = [row.created_utc for row in batch_data if row.created_utc]\n",
    "        time_range = {\n",
    "            'start': min(created_times) if created_times else 0,\n",
    "            'end': max(created_times) if created_times else 0,\n",
    "            'span_minutes': (max(created_times) - min(created_times)) / 60 if len(created_times) > 1 else 0\n",
    "        }\n",
    "        \n",
    "        # Create metrics record\n",
    "        metrics = {\n",
    "            'batch_id': batch_id,\n",
    "            'timestamp': current_time.isoformat(),\n",
    "            'total_comments': len(batch_data),\n",
    "            'avg_sentiment': avg_sentiment,\n",
    "            'sentiment_distribution': {\n",
    "                'positive': positive_comments,\n",
    "                'negative': negative_comments,\n",
    "                'neutral': neutral_comments\n",
    "            },\n",
    "            'avg_score': avg_score,\n",
    "            'total_words': total_words,\n",
    "            'avg_words_per_comment': total_words / len(batch_data) if batch_data else 0,\n",
    "            'subreddit_distribution': dict(sorted(subreddit_counts.items(), key=lambda x: x[1], reverse=True)[:5]),\n",
    "            'top_words_tfidf': top_words_tfidf[:10],\n",
    "            'references': references,\n",
    "            'time_range': time_range,\n",
    "            'processing_stats': {\n",
    "                'total_batches_processed': batch_counter,\n",
    "                'total_comments_accumulated': len(raw_data_buffer)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metrics_buffer.append(metrics)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"üìä Metrics Summary:\")\n",
    "        print(f\"   üí¨ Comments: {len(batch_data)}\")\n",
    "        print(f\"   üòä Avg Sentiment: {avg_sentiment:.3f}\")\n",
    "        print(f\"   ‚≠ê Avg Score: {avg_score:.1f}\")\n",
    "        print(f\"   üìù Total Words: {total_words}\")\n",
    "        print(f\"   üè∑Ô∏è  Top Subreddits: {list(subreddit_counts.keys())[:3]}\")\n",
    "        if top_words_tfidf:\n",
    "            print(f\"   üî§ Top Words: {[word for word, count in top_words_tfidf[:5]]}\")\n",
    "        print(f\"   üë• User Mentions: {len(references['top_user_mentions'])}\")\n",
    "        print(f\"   üîó URLs: {len(references['top_url_domains'])}\")\n",
    "        \n",
    "        # Save data every 10 batches\n",
    "        if batch_counter % 10 == 0:\n",
    "            save_data_to_files()\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in calculate_metrics: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_data_to_files():\n",
    "    \"\"\"Save accumulated data to files\"\"\"\n",
    "    try:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Save raw data\n",
    "        if raw_data_buffer:\n",
    "            raw_df = spark.createDataFrame(raw_data_buffer)\n",
    "            raw_df.write.mode('append').json(f\"output/raw_data/raw_data_{timestamp}\")\n",
    "            raw_df.createOrReplaceTempView(\"raw\")\n",
    "            print(f\"üíæ Raw data saved: {len(raw_data_buffer)} records\")\n",
    "        \n",
    "        # Save metrics\n",
    "        if metrics_buffer:\n",
    "            with open(f\"output/metrics/metrics_{timestamp}.json\", 'w') as f:\n",
    "                json.dump(metrics_buffer, f, indent=2)\n",
    "            \n",
    "            # Create Spark DataFrame for metrics\n",
    "            metrics_df = spark.createDataFrame([json.dumps(m) for m in metrics_buffer], StringType())\n",
    "            metrics_df.createOrReplaceTempView(\"metrics\")\n",
    "            metrics_df.write.mode('append').text(f\"output/metrics/spark_metrics_{timestamp}\")\n",
    "            print(f\"üìà Metrics saved: {len(metrics_buffer)} records\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving data: {e}\")\n",
    "\n",
    "print(\"üîß Processing functions defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e02ea-fa2f-4df5-8c22-9f5831f83a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main streaming processing function\n",
    "def process_batch(df, epoch_id):\n",
    "    \"\"\"Process each batch of streaming data\"\"\"\n",
    "    try:\n",
    "        if df.count() == 0:\n",
    "            print(f\"üì≠ Batch {epoch_id}: No data\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"üîÑ PROCESSING BATCH {epoch_id}\")\n",
    "        print(f\"üìÖ Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Calculate and display metrics\n",
    "        metrics = calculate_metrics(df, epoch_id)\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nüìã Sample Comments:\")\n",
    "        sample_data = df.limit(3).collect()\n",
    "        for i, row in enumerate(sample_data, 1):\n",
    "            sentiment_emoji = \"üòä\" if row.sentiment.compound > 0.1 else \"üòû\" if row.sentiment.compound < -0.1 else \"üòê\"\n",
    "            print(f\"   {i}. {sentiment_emoji} [{row.subreddit}] {row.text[:100]}...\")\n",
    "            print(f\"      Score: {row.score}, Sentiment: {row.sentiment.compound:.3f}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing batch {epoch_id}: {e}\")\n",
    "\n",
    "# Start the streaming query\n",
    "print(\"üöÄ Starting streaming query...\")\n",
    "print(\"üì° Waiting for data from Reddit producer...\")\n",
    "print(\"üõë Press Ctrl+C to stop\\n\")\n",
    "\n",
    "query = processed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"./checkpoint\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"‚úÖ Streaming query started!\")\n",
    "print(f\"üîç Query ID: {query.id}\")\n",
    "print(f\"üìä Status: {query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3847f6-cf61-4f19-8456-05b5a6c38fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Session 6 Spark streaming intro\n",
    "\n",
    "# Monitoring and control functions\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def monitor_stream():\n",
    "    \"\"\"Monitor the streaming query status\"\"\"\n",
    "    while query.isActive:\n",
    "        try:\n",
    "            print(f\"\\nüìä STREAM STATUS - {datetime.now().strftime('%H:%M:%S')}\")\n",
    "            print(f\"   üîÑ Active: {query.status['isDataAvailable']}\")\n",
    "            print(f\"   üìà Message: {query.status['message']}\")\n",
    "            print(f\"   üìä Batches: {batch_counter}\")\n",
    "            print(f\"   üíæ Raw Data Buffer: {len(raw_data_buffer)} records\")\n",
    "            print(f\"   üìà Metrics Buffer: {len(metrics_buffer)} records\")\n",
    "            \n",
    "            if len(raw_data_buffer) > 0:\n",
    "                recent_sentiment = sum([r.sentiment.compound for r in raw_data_buffer[-10:] if r.sentiment]) / min(10, len(raw_data_buffer))\n",
    "                print(f\"   üòä Recent Avg Sentiment: {recent_sentiment:.3f}\")\n",
    "            \n",
    "            time.sleep(30)  # Check every 30 seconds\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Monitor error: {e}\")\n",
    "            break\n",
    "\n",
    "# Start monitoring in background\n",
    "monitor_thread = threading.Thread(target=monitor_stream, daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "print(\"üìä Stream monitoring started in background\")\n",
    "print(\"‚è∞ Status updates every 30 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccd01f-de29-42db-96ce-597a99c2a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for stream termination\n",
    "try:\n",
    "    print(\"\\nüéØ Stream is running! Processing Reddit comments...\")\n",
    "    print(\"üìä Check the output above for real-time metrics\")\n",
    "    print(\"üíæ Data is being saved to output/ directory\")\n",
    "    print(\"üõë Press Ctrl+C to stop gracefully\\n\")\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Stopping stream gracefully...\")\n",
    "    query.stop()\n",
    "    \n",
    "    # Final save\n",
    "    save_data_to_files()\n",
    "    \n",
    "    print(\"\\nüìã FINAL SUMMARY:\")\n",
    "    print(f\"üìä Total Batches Processed: {batch_counter}\")\n",
    "    print(f\"üí¨ Total Comments Processed: {len(raw_data_buffer)}\")\n",
    "    print(f\"üìà Total Metrics Records: {len(metrics_buffer)}\")\n",
    "    print(f\"üíæ Data saved to output/ directory\")\n",
    "    \n",
    "    if len(raw_data_buffer) > 0:\n",
    "        avg_sentiment = sum([r.sentiment.compound for r in raw_data_buffer if r.sentiment]) / len([r for r in raw_data_buffer if r.sentiment])\n",
    "        print(f\"üòä Overall Average Sentiment: {avg_sentiment:.3f}\")\n",
    "        \n",
    "        subreddit_counts = {}\n",
    "        for r in raw_data_buffer:\n",
    "            if r.subreddit:\n",
    "                subreddit_counts[r.subreddit] = subreddit_counts.get(r.subreddit, 0) + 1\n",
    "        top_subreddits = sorted(subreddit_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(f\"üè∑Ô∏è  Top Subreddits: {[sub for sub, count in top_subreddits]}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Stream stopped successfully!\")\n",
    "    \n",
    "finally:\n",
    "    spark.stop()\n",
    "    print(\"üî• Spark session stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7937f52-3db5-4390-8b7a-c00905afb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "def create_visualizations():\n",
    "    \"\"\"Create comprehensive visualizations of the processed data\"\"\"\n",
    "    try:\n",
    "        if not metrics_buffer:\n",
    "            print(\"‚ùå No metrics data available for visualization\")\n",
    "            return\n",
    "        \n",
    "        print(\"üìä Creating visualizations...\")\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        # 1. Sentiment Analysis Over Time\n",
    "        ax1 = plt.subplot(3, 3, 1)\n",
    "        sentiments = [m['avg_sentiment'] for m in metrics_buffer]\n",
    "        batches = [m['batch_id'] for m in metrics_buffer]\n",
    "        plt.plot(batches, sentiments, 'b-o', linewidth=2, markersize=4)\n",
    "        plt.title('Average Sentiment Over Time', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Batch ID')\n",
    "        plt.ylabel('Sentiment Score')\n",
    "        plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Comments Count Over Time\n",
    "        ax2 = plt.subplot(3, 3, 2)\n",
    "        comment_counts = [m['total_comments'] for m in metrics_buffer]\n",
    "        plt.bar(batches, comment_counts, color='skyblue', alpha=0.7)\n",
    "        plt.title('Comments Per Batch', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Batch ID')\n",
    "        plt.ylabel('Number of Comments')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Sentiment Distribution (Latest Batch)\n",
    "        ax3 = plt.subplot(3, 3, 3)\n",
    "        if metrics_buffer:\n",
    "            latest_sentiment = metrics_buffer[-1]['sentiment_distribution']\n",
    "            labels = ['Positive', 'Negative', 'Neutral']\n",
    "            sizes = [latest_sentiment['positive'], latest_sentiment['negative'], latest_sentiment['neutral']]\n",
    "            colors = ['#90EE90', '#FFB6C1', '#D3D3D3']\n",
    "            plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "            plt.title('Sentiment Distribution (Latest Batch)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 4. Average Score Over Time\n",
    "        ax4 = plt.subplot(3, 3, 4)\n",
    "        avg_scores = [m['avg_score'] for m in metrics_buffer]\n",
    "        plt.plot(batches, avg_scores, 'g-o', linewidth=2, markersize=4)\n",
    "        plt.title('Average Reddit Score Over Time', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Batch ID')\n",
    "        plt.ylabel('Average Score')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Words Per Comment\n",
    "        ax5 = plt.subplot(3, 3, 5)\n",
    "        words_per_comment = [m['avg_words_per_comment'] for m in metrics_buffer]\n",
    "        plt.plot(batches, words_per_comment, 'purple', linewidth=2, marker='s', markersize=4)\n",
    "        plt.title('Average Words Per Comment', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Batch ID')\n",
    "        plt.ylabel('Words per Comment')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Top Subreddits (Latest Batch)\n",
    "        ax6 = plt.subplot(3, 3, 6)\n",
    "        if metrics_buffer and metrics_buffer[-1]['subreddit_distribution']:\n",
    "            subreddit_data = metrics_buffer[-1]['subreddit_distribution']\n",
    "            subs = list(subreddit_data.keys())[:5]\n",
    "            counts = list(subreddit_data.values())[:5]\n",
    "            plt.barh(subs, counts, color='orange', alpha=0.7)\n",
    "            plt.title('Top Subreddits (Latest Batch)', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Number of Comments')\n",
    "        \n",
    "        # 7. TF-IDF Top Words (Latest Batch)\n",
    "        ax7 = plt.subplot(3, 3, 7)\n",
    "        if metrics_buffer and metrics_buffer[-1]['top_words_tfidf']:\n",
    "            words_data = metrics_buffer[-1]['top_words_tfidf'][:10]\n",
    "            if words_data:\n",
    "                words = [item[0] for item in words_data]\n",
    "                freqs = [item[1] for item in words_data]\n",
    "                plt.barh(words, freqs, color='red', alpha=0.7)\n",
    "                plt.title('Top Words (TF-IDF)', fontsize=14, fontweight='bold')\n",
    "                plt.xlabel('Frequency')\n",
    "        \n",
    "        # 8. Processing Statistics\n",
    "        ax8 = plt.subplot(3, 3, 8)\n",
    "        total_comments = [m['processing_stats']['total_comments_accumulated'] for m in metrics_buffer]\n",
    "        plt.plot(batches, total_comments, 'brown', linewidth=3, marker='D', markersize=4)\n",
    "        plt.title('Cumulative Comments Processed', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Batch ID')\n",
    "        plt.ylabel('Total Comments')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 9. Reference Analysis (Latest Batch)\n",
    "        ax9 = plt.subplot(3, 3, 9)\n",
    "        if metrics_buffer and metrics_buffer[-1]['references']:\n",
    "            ref_data = metrics_buffer[-1]['references']\n",
    "            ref_types = ['User Mentions', 'Subreddit Refs', 'URL Domains']\n",
    "            ref_counts = [\n",
    "                len(ref_data['top_user_mentions']),\n",
    "                len(ref_data['top_subreddit_refs']),\n",
    "                len(ref_data['top_url_domains'])\n",
    "            ]\n",
    "            plt.bar(ref_types, ref_counts, color=['blue', 'green', 'red'], alpha=0.7)\n",
    "            plt.title('References Found (Latest Batch)', fontsize=14, fontweight='bold')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        plt.savefig(f'output/visualizations/reddit_analytics_{timestamp}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üìä Visualizations saved to output/visualizations/reddit_analytics_{timestamp}.png\")\n",
    "        \n",
    "        # Create Word Cloud if we have text data\n",
    "        create_wordcloud()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating visualizations: {e}\")\n",
    "\n",
    "def create_wordcloud():\n",
    "    \"\"\"Create word cloud from all processed text\"\"\"\n",
    "    try:\n",
    "        if not raw_data_buffer:\n",
    "            return\n",
    "        \n",
    "        # Combine all text\n",
    "        all_text = \" \".join([row.text for row in raw_data_buffer if row.text])\n",
    "        \n",
    "        if len(all_text) < 100:\n",
    "            print(\"‚ùå Not enough text for word cloud\")\n",
    "            return\n",
    "        \n",
    "        # Create word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, \n",
    "                            background_color='white',\n",
    "                            colormap='viridis',\n",
    "                            max_words=100).generate(all_text)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud - Germany-related Reddit Comments', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        plt.savefig(f'output/visualizations/wordcloud_{timestamp}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"‚òÅÔ∏è Word cloud saved to output/visualizations/wordcloud_{timestamp}.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating word cloud: {e}\")\n",
    "\n",
    "def generate_summary_report():\n",
    "    \"\"\"Generate a comprehensive summary report\"\"\"\n",
    "    try:\n",
    "        if not metrics_buffer:\n",
    "            print(\"‚ùå No data available for report\")\n",
    "            return\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# Reddit Germany Analytics Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Overview\n",
    "- Total Batches Processed: {len(metrics_buffer)}\n",
    "- Total Comments Analyzed: {len(raw_data_buffer)}\n",
    "- Analysis Period: {metrics_buffer[0]['timestamp'] if metrics_buffer else 'N/A'} to {metrics_buffer[-1]['timestamp'] if metrics_buffer else 'N/A'}\n",
    "\n",
    "## Key Metrics\n",
    "\"\"\"\n",
    "        \n",
    "        if metrics_buffer:\n",
    "            avg_sentiment_overall = sum([m['avg_sentiment'] for m in metrics_buffer]) / len(metrics_buffer)\n",
    "            avg_score_overall = sum([m['avg_score'] for m in metrics_buffer]) / len(metrics_buffer)\n",
    "            total_comments = sum([m['total_comments'] for m in metrics_buffer])\n",
    "            \n",
    "            report += f\"\"\"\n",
    "- Overall Average Sentiment: {avg_sentiment_overall:.3f}\n",
    "- Overall Average Score: {avg_score_overall:.2f}\n",
    "- Total Comments Processed: {total_comments}\n",
    "- Average Comments per Batch: {total_comments / len(metrics_buffer):.1f}\n",
    "\n",
    "## Sentiment Analysis\n",
    "\"\"\"\n",
    "            \n",
    "            # Sentiment summary\n",
    "            total_positive = sum([m['sentiment_distribution']['positive'] for m in metrics_buffer])\n",
    "            total_negative = sum([m['sentiment_distribution']['negative'] for m in metrics_buffer])\n",
    "            total_neutral = sum([m['sentiment_distribution']['neutral'] for m in metrics_buffer])\n",
    "            \n",
    "            report += f\"\"\"\n",
    "- Positive Comments: {total_positive} ({total_positive/total_comments*100:.1f}%)\n",
    "- Negative Comments: {total_negative} ({total_negative/total_comments*100:.1f}%)\n",
    "- Neutral Comments: {total_neutral} ({total_neutral/total_comments*100:.1f}%)\n",
    "\n",
    "## Most Recent Analysis\n",
    "\"\"\"\n",
    "            \n",
    "            latest = metrics_buffer[-1]\n",
    "            report += f\"\"\"\n",
    "- Latest Batch: {latest['batch_id']}\n",
    "- Comments in Latest Batch: {latest['total_comments']}\n",
    "- Latest Average Sentiment: {latest['avg_sentiment']:.3f}\n",
    "- Top Words (TF-IDF): {[word for word, freq in latest['top_words_tfidf'][:5]]}\n",
    "\"\"\"\n",
    "            \n",
    "            if latest['references']['top_user_mentions']:\n",
    "                report += f\"\\n- Top User Mentions: {[user for user, count in latest['references']['top_user_mentions'][:5]]}\"\n",
    "            \n",
    "            if latest['references']['top_subreddit_refs']:\n",
    "                report += f\"\\n- Top Subreddit References: {[sub for sub, count in latest['references']['top_subreddit_refs'][:5]]}\"\n",
    "        \n",
    "        # Save report\n",
    "        with open(f'output/summary_report_{timestamp}.md', 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"üìã Summary report saved to output/summary_report_{timestamp}.md\")\n",
    "        print(\"\\n\" + report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating report: {e}\")\n",
    "\n",
    "print(\"üìä Visualization functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3509fe-626f-416f-9a69-678ac7bc70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions for Data Export and Analysis\n",
    "\n",
    "def export_to_csv():\n",
    "    \"\"\"Export processed data to CSV format\"\"\"\n",
    "    try:\n",
    "        if not raw_data_buffer:\n",
    "            print(\"‚ùå No raw data to export\")\n",
    "            return\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Convert raw data to pandas DataFrame\n",
    "        data_for_csv = []\n",
    "        for row in raw_data_buffer:\n",
    "            data_for_csv.append({\n",
    "                'id': row.id,\n",
    "                'text': row.text[:500] if row.text else '',  # Truncate long text\n",
    "                'created_utc': row.created_utc,\n",
    "                'author': row.author,\n",
    "                'subreddit': row.subreddit,\n",
    "                'score': row.score,\n",
    "                'sentiment_compound': row.sentiment.compound if row.sentiment else 0,\n",
    "                'sentiment_positive': row.sentiment.positive if row.sentiment else 0,\n",
    "                'sentiment_negative': row.sentiment.negative if row.sentiment else 0,\n",
    "                'word_count': row.word_count,\n",
    "                'char_count': row.char_count,\n",
    "                'user_mentions_count': len(row.user_mentions) if row.user_mentions else 0,\n",
    "                'url_count': len(row.urls) if row.urls else 0\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data_for_csv)\n",
    "        csv_path = f'output/reddit_data_{timestamp}.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"üìÑ Data exported to CSV: {csv_path}\")\n",
    "        print(f\"üìä Exported {len(df)} records\")\n",
    "        \n",
    "        # Also export metrics\n",
    "        if metrics_buffer:\n",
    "            metrics_df = pd.DataFrame(metrics_buffer)\n",
    "            metrics_csv_path = f'output/metrics_{timestamp}.csv'\n",
    "            metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "            print(f\"üìà Metrics exported to CSV: {metrics_csv_path}\")\n",
    "        \n",
    "        return csv_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting to CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_peak_activity():\n",
    "    \"\"\"Analyze peak activity periods\"\"\"\n",
    "    try:\n",
    "        if not raw_data_buffer:\n",
    "            print(\"‚ùå No data for peak analysis\")\n",
    "            return\n",
    "        \n",
    "        # Group by hour\n",
    "        hourly_activity = {}\n",
    "        for row in raw_data_buffer:\n",
    "            if row.created_utc:\n",
    "                hour = datetime.fromtimestamp(row.created_utc).hour\n",
    "                hourly_activity[hour] = hourly_activity.get(hour, 0) + 1\n",
    "        \n",
    "        # Find peak hours\n",
    "        sorted_hours = sorted(hourly_activity.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\nüïê Peak Activity Analysis:\")\n",
    "        print(f\"üìä Total hours with activity: {len(hourly_activity)}\")\n",
    "        if sorted_hours:\n",
    "            print(f\"üî• Peak hour: {sorted_hours[0][0]}:00 ({sorted_hours[0][1]} comments)\")\n",
    "            print(f\"üìà Top 5 active hours:\")\n",
    "            for hour, count in sorted_hours[:5]:\n",
    "                print(f\"   {hour:2d}:00 - {count:3d} comments\")\n",
    "        \n",
    "        return hourly_activity\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing peak activity: {e}\")\n",
    "        return {}\n",
    "\n",
    "def get_top_controversial():\n",
    "    \"\"\"Get most controversial comments (high engagement, mixed sentiment)\"\"\"\n",
    "    try:\n",
    "        if not raw_data_buffer:\n",
    "            return []\n",
    "        \n",
    "        controversial = []\n",
    "        for row in raw_data_buffer:\n",
    "            if row.score and row.sentiment and row.text:\n",
    "                # High score + neutral sentiment = controversial\n",
    "                controversy_score = abs(row.score) * (1 - abs(row.sentiment.compound))\n",
    "                controversial.append({\n",
    "                    'text': row.text[:200],\n",
    "                    'score': row.score,\n",
    "                    'sentiment': row.sentiment.compound,\n",
    "                    'controversy_score': controversy_score,\n",
    "                    'subreddit': row.subreddit,\n",
    "                    'author': row.author\n",
    "                })\n",
    "        \n",
    "        # Sort by controversy score\n",
    "        controversial.sort(key=lambda x: x['controversy_score'], reverse=True)\n",
    "        \n",
    "        print(\"\\nüî• Most Controversial Comments:\")\n",
    "        for i, comment in enumerate(controversial[:5], 1):\n",
    "            print(f\"{i}. Score: {comment['score']:3d}, Sentiment: {comment['sentiment']:+.3f}\")\n",
    "            print(f\"   [{comment['subreddit']}] {comment['text'][:150]}...\")\n",
    "            print()\n",
    "        \n",
    "        return controversial[:10]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error finding controversial comments: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"üîß Utility functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e2ee6-f331-492a-83b8-2e276aec9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Execution Section\n",
    "# This cell should be run after the streaming has been stopped\n",
    "\n",
    "def run_final_analysis():\n",
    "    \"\"\"Run comprehensive final analysis after streaming stops\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üî¨ RUNNING FINAL COMPREHENSIVE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not raw_data_buffer and not metrics_buffer:\n",
    "        print(\"‚ùå No data collected. Make sure to run the producer first!\")\n",
    "        return\n",
    "    \n",
    "    # 1. Generate visualizations\n",
    "    print(\"\\nüìä 1. Creating Visualizations...\")\n",
    "    create_visualizations()\n",
    "    \n",
    "    # 2. Export data\n",
    "    print(\"\\nüíæ 2. Exporting Data...\")\n",
    "    csv_path = export_to_csv()\n",
    "    \n",
    "    # 3. Peak activity analysis\n",
    "    print(\"\\nüïê 3. Analyzing Peak Activity...\")\n",
    "    hourly_data = analyze_peak_activity()\n",
    "    \n",
    "    # 4. Find controversial content\n",
    "    print(\"\\nüî• 4. Finding Controversial Content...\")\n",
    "    controversial = get_top_controversial()\n",
    "    \n",
    "    # 5. Generate summary report\n",
    "    print(\"\\nüìã 5. Generating Summary Report...\")\n",
    "    generate_summary_report()\n",
    "    \n",
    "    # 6. Final statistics\n",
    "    print(\"\\nüìä 6. Final Statistics Summary:\")\n",
    "    if raw_data_buffer:\n",
    "        print(f\"   üí¨ Total Comments: {len(raw_data_buffer)}\")\n",
    "        print(f\"   üìà Total Batches: {len(metrics_buffer)}\")\n",
    "        \n",
    "        avg_sentiment = sum([r.sentiment.compound for r in raw_data_buffer if r.sentiment]) / len([r for r in raw_data_buffer if r.sentiment])\n",
    "        print(f\"   üòä Overall Sentiment: {avg_sentiment:.3f}\")\n",
    "        \n",
    "        subreddit_counts = {}\n",
    "        for r in raw_data_buffer:\n",
    "            if r.subreddit:\n",
    "                subreddit_counts[r.subreddit] = subreddit_counts.get(r.subreddit, 0) + 1\n",
    "        \n",
    "        print(f\"   üè∑Ô∏è  Subreddits Covered: {len(subreddit_counts)}\")\n",
    "        print(f\"   üîù Top Subreddit: {max(subreddit_counts.items(), key=lambda x: x[1]) if subreddit_counts else 'None'}\")\n",
    "        \n",
    "        total_words = sum([r.word_count for r in raw_data_buffer if r.word_count])\n",
    "        print(f\"   üìù Total Words Analyzed: {total_words:,}\")\n",
    "        \n",
    "        url_count = sum([len(r.urls) for r in raw_data_buffer if r.urls])\n",
    "        user_mention_count = sum([len(r.user_mentions) for r in raw_data_buffer if r.user_mentions])\n",
    "        print(f\"   üîó URLs Found: {url_count}\")\n",
    "        print(f\"   üë• User Mentions: {user_mention_count}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Final analysis complete!\")\n",
    "    print(f\"üìÅ All results saved in output/ directory\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Instructions for running the analysis\n",
    "print(\"\"\"\\nüìù INSTRUCTIONS FOR FINAL ANALYSIS:\n",
    "\n",
    "After stopping the streaming query (Ctrl+C), run:\n",
    "    run_final_analysis()\n",
    "\n",
    "This will:\n",
    "‚úÖ Create comprehensive visualizations\n",
    "‚úÖ Export data to CSV format\n",
    "‚úÖ Analyze peak activity periods\n",
    "‚úÖ Find controversial comments\n",
    "‚úÖ Generate summary report\n",
    "‚úÖ Show final statistics\n",
    "\n",
    "All results will be saved in the output/ directory.\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéØ Ready for final analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3085f6-666f-4e75-b3f4-be821e1a17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting and Help Section\n",
    "\n",
    "def show_help():\n",
    "    \"\"\"Display help information\"\"\"\n",
    "    help_text = \"\"\"\n",
    "üÜò REDDIT ANALYTICS TROUBLESHOOTING GUIDE\n",
    "\n",
    "‚ùì COMMON ISSUES:\n",
    "\n",
    "1. \"No data received\" or \"Batch X: No data\"\n",
    "   ‚úÖ Make sure the Reddit Producer is running first\n",
    "   ‚úÖ Check that both producer and consumer use the same port (9998)\n",
    "   ‚úÖ Verify Reddit API credentials are valid\n",
    "   ‚úÖ Ensure you're connected to the internet\n",
    "\n",
    "2. \"Connection refused\" errors\n",
    "   ‚úÖ Start the consumer first, then the producer\n",
    "   ‚úÖ Check if port 9998 is available\n",
    "   ‚úÖ Try changing the port in both producer and consumer\n",
    "\n",
    "3. \"TF-IDF\" or \"Spark\" errors\n",
    "   ‚úÖ Make sure PySpark is properly installed\n",
    "   ‚úÖ Check Java version (Java 8 or 11 recommended)\n",
    "   ‚úÖ Verify Spark dependencies\n",
    "\n",
    "4. Visualization errors\n",
    "   ‚úÖ Install required packages: matplotlib, seaborn, wordcloud\n",
    "   ‚úÖ Run: pip install matplotlib seaborn wordcloud\n",
    "\n",
    "üöÄ RUNNING THE PROJECT:\n",
    "\n",
    "1. Start Consumer (this notebook):\n",
    "   - Run all cells up to the streaming query\n",
    "   - Wait for \"Waiting for data from Reddit producer...\" message\n",
    "\n",
    "2. Start Producer (other notebook):\n",
    "   - Make sure Reddit API credentials are set\n",
    "   - Run the streaming cell\n",
    "   - Look for \"üîº Sending comment to socket\" messages\n",
    "\n",
    "3. Monitor Progress:\n",
    "   - Watch for batch processing messages in consumer\n",
    "   - Check output/ directory for saved files\n",
    "   - Stream status updates every 30 seconds\n",
    "\n",
    "4. Stop and Analyze:\n",
    "   - Press Ctrl+C to stop streaming\n",
    "   - Run run_final_analysis() for comprehensive results\n",
    "\n",
    "üìä OUTPUT FILES:\n",
    "- output/raw_data/ - Raw Reddit comments\n",
    "- output/metrics/ - Processed analytics\n",
    "- output/visualizations/ - Charts and graphs\n",
    "- output/*.csv - Data in CSV format\n",
    "- output/summary_report_*.md - Summary report\n",
    "\n",
    "üîß CONFIGURATION:\n",
    "- Port: 9998 (change in both notebooks if needed)\n",
    "- Batch processing: Every 10 seconds\n",
    "- Data buffer: Last 1000 comments\n",
    "- Window analysis: 60 seconds\n",
    "\n",
    "üìû SUPPORT:\n",
    "If issues persist, check:\n",
    "- Python version (3.7+ recommended)\n",
    "- PySpark installation\n",
    "- Network connectivity\n",
    "- Reddit API status\n",
    "\"\"\"\n",
    "    print(help_text)\n",
    "\n",
    "def show_current_status():\n",
    "    \"\"\"Show current processing status\"\"\"\n",
    "    print(f\"\\nüìä CURRENT STATUS - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"üìà Batches Processed: {batch_counter}\")\n",
    "    print(f\"üíæ Raw Data Buffer: {len(raw_data_buffer)} records\")\n",
    "    print(f\"üìä Metrics Buffer: {len(metrics_buffer)} records\")\n",
    "    \n",
    "    if raw_data_buffer:\n",
    "        recent_comments = raw_data_buffer[-5:]\n",
    "        print(f\"\\nüî• Recent Comments:\")\n",
    "        for i, comment in enumerate(recent_comments, 1):\n",
    "            sentiment_emoji = \"üòä\" if comment.sentiment.compound > 0.1 else \"üòû\" if comment.sentiment.compound < -0.1 else \"üòê\"\n",
    "            print(f\"   {i}. {sentiment_emoji} [{comment.subreddit}] {comment.text[:80]}...\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Output Directory Contents:\")\n",
    "    try:\n",
    "        for root, dirs, files in os.walk(\"output\"):\n",
    "            level = root.replace(\"output\", \"\").count(os.sep)\n",
    "            indent = \" \" * 2 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            subindent = \" \" * 2 * (level + 1)\n",
    "            for file in files:\n",
    "                print(f\"{subindent}{file}\")\n",
    "    except:\n",
    "        print(\"   No output directory found yet\")\n",
    "\n",
    "# Quick access functions\n",
    "print(\"\\nüÜò HELP FUNCTIONS AVAILABLE:\")\n",
    "print(\"   show_help() - Display troubleshooting guide\")\n",
    "print(\"   show_current_status() - Show current processing status\")\n",
    "print(\"   run_final_analysis() - Run comprehensive analysis\")\n",
    "print(\"   create_visualizations() - Create charts and graphs\")\n",
    "print(\"   export_to_csv() - Export data to CSV format\")\n",
    "print(\"\\nüí° Type show_help() if you need assistance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21385038-5f25-499a-974f-bf9b0d536311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell was cleaned up - content moved to proper sections above# üéâ REDDIT GERMANY ANALYTICS - COMPLETE SETUP!\n",
    "# \n",
    "# This notebook provides comprehensive real-time analytics for Germany-related\n",
    "# Reddit discussions with the following features:\n",
    "#\n",
    "# ‚úÖ Real-time data streaming via socket\n",
    "# ‚úÖ Comprehensive data processing with Spark\n",
    "# ‚úÖ TF-IDF analysis for important words\n",
    "# ‚úÖ Reference extraction (users, subreddits, URLs)\n",
    "# ‚úÖ Sentiment analysis with VADER\n",
    "# ‚úÖ Real-time metrics and statistics\n",
    "# ‚úÖ Data persistence (JSON, CSV, Parquet)\n",
    "# ‚úÖ Comprehensive visualizations\n",
    "# ‚úÖ Automated reporting\n",
    "# ‚úÖ Error handling and monitoring\n",
    "#\n",
    "# üìä Analytics Features:\n",
    "# - Sentiment analysis over time\n",
    "# - Comment volume tracking\n",
    "# - TF-IDF word importance\n",
    "# - Peak activity analysis\n",
    "# - Controversial content detection\n",
    "# - Subreddit distribution\n",
    "# - Reference network analysis\n",
    "#\n",
    "# üéØ Ready to analyze Germany-related Reddit discussions in real-time!\n",
    "\n",
    "print(\"\\nüéâ REDDIT ANALYTICS CONSUMER READY!\")\n",
    "print(\"üìä All features loaded successfully!\")\n",
    "print(\"üöÄ Start the streaming query above to begin processing!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
